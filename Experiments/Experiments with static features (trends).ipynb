{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFeWReVeGBVA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, bz2, json, time\n",
    "from datetime import timedelta\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "from PIL import Image\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "dNdmC-k6GBVF"
   },
   "outputs": [],
   "source": [
    "# data_en = pd.read_csv('Data3_en.csv')\n",
    "# data_ru = pd.read_csv('Data3_ru.csv')\n",
    "data = data_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "-S5sK1dWGBVG"
   },
   "outputs": [],
   "source": [
    "replacer = dict()\n",
    "\n",
    "with open('category-codes.txt') as f:\n",
    "    categories = f.read().splitlines()\n",
    "    for cat in categories:\n",
    "        replacer[int(cat.split(' - ')[0])] = cat.split(' - ')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "sdlWh54iGBVG",
    "outputId": "d0fab040-86d2-47d0-8e73-0dd4f7461ca8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-376b7e31-b9ce-444c-bf46-4c89f8c941cc\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>channelId</th>\n",
       "      <th>channelTitle</th>\n",
       "      <th>categoryId</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>tags</th>\n",
       "      <th>view_count</th>\n",
       "      <th>likes</th>\n",
       "      <th>...</th>\n",
       "      <th>Day1</th>\n",
       "      <th>Day2</th>\n",
       "      <th>Day3</th>\n",
       "      <th>Day4</th>\n",
       "      <th>Day5</th>\n",
       "      <th>Day6</th>\n",
       "      <th>Day7</th>\n",
       "      <th>Day8</th>\n",
       "      <th>Day9</th>\n",
       "      <th>Day10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23753</th>\n",
       "      <td>Jsn5V9o7AlE</td>\n",
       "      <td>Doston Ergashev - Dada | Достон Эргашев - Дада...</td>\n",
       "      <td>2021-11-24T12:00:20Z</td>\n",
       "      <td>UCXhlq_EYf205l29vpY254qw</td>\n",
       "      <td>RizaNovaUZ</td>\n",
       "      <td>Music</td>\n",
       "      <td>2021-11-26</td>\n",
       "      <td>uzbek, узбек, klip, клип, yangi, янги, uzbek k...</td>\n",
       "      <td>2</td>\n",
       "      <td>4872</td>\n",
       "      <td>...</td>\n",
       "      <td>58873</td>\n",
       "      <td>335642.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39174</th>\n",
       "      <td>XY9hFMK0n60</td>\n",
       "      <td>Erdoğan dünya liderlerine seslendi</td>\n",
       "      <td>2021-05-16T16:45:48Z</td>\n",
       "      <td>UC7DDcxd92wR0LzlMKIcHDAA</td>\n",
       "      <td>Show Ana Haber</td>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>showtv, güncel, gündem, showhaber, show haber,...</td>\n",
       "      <td>1</td>\n",
       "      <td>2230</td>\n",
       "      <td>...</td>\n",
       "      <td>81853</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72344</th>\n",
       "      <td>zht1CEaYXKE</td>\n",
       "      <td>Nilufar Usmonova - Bojxonachilar | Нилуфар Усм...</td>\n",
       "      <td>2022-01-28T10:34:52Z</td>\n",
       "      <td>UCDPnSqLmYqHadD6VkrRiarg</td>\n",
       "      <td>RizaNovaUZ</td>\n",
       "      <td>Music</td>\n",
       "      <td>2022-01-29</td>\n",
       "      <td>uzbek, узбек, klip, клип, yangi, янги, music, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>4524</td>\n",
       "      <td>...</td>\n",
       "      <td>223233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 39 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-376b7e31-b9ce-444c-bf46-4c89f8c941cc')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-376b7e31-b9ce-444c-bf46-4c89f8c941cc button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-376b7e31-b9ce-444c-bf46-4c89f8c941cc');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "          video_id                                              title  \\\n",
       "23753  Jsn5V9o7AlE  Doston Ergashev - Dada | Достон Эргашев - Дада...   \n",
       "39174  XY9hFMK0n60               Erdoğan dünya liderlerine seslendi   \n",
       "72344  zht1CEaYXKE  Nilufar Usmonova - Bojxonachilar | Нилуфар Усм...   \n",
       "\n",
       "                publishedAt                 channelId    channelTitle  \\\n",
       "23753  2021-11-24T12:00:20Z  UCXhlq_EYf205l29vpY254qw      RizaNovaUZ   \n",
       "39174  2021-05-16T16:45:48Z  UC7DDcxd92wR0LzlMKIcHDAA  Show Ana Haber   \n",
       "72344  2022-01-28T10:34:52Z  UCDPnSqLmYqHadD6VkrRiarg      RizaNovaUZ   \n",
       "\n",
       "            categoryId trending_date  \\\n",
       "23753            Music    2021-11-26   \n",
       "39174  News & Politics    2021-05-17   \n",
       "72344            Music    2022-01-29   \n",
       "\n",
       "                                                    tags  view_count  likes  \\\n",
       "23753  uzbek, узбек, klip, клип, yangi, янги, uzbek k...           2   4872   \n",
       "39174  showtv, güncel, gündem, showhaber, show haber,...           1   2230   \n",
       "72344  uzbek, узбек, klip, клип, yangi, янги, music, ...           1   4524   \n",
       "\n",
       "       ...    Day1      Day2 Day3  Day4  Day5 Day6 Day7 Day8 Day9  Day10  \n",
       "23753  ...   58873  335642.0  NaN   NaN   NaN  NaN  NaN  NaN  NaN    NaN  \n",
       "39174  ...   81853       NaN  NaN   NaN   NaN  NaN  NaN  NaN  NaN    NaN  \n",
       "72344  ...  223233       NaN  NaN   NaN   NaN  NaN  NaN  NaN  NaN    NaN  \n",
       "\n",
       "[3 rows x 39 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_columns = ['title', 'description', 'tags']\n",
    "categorical_columns = ['categoryId', 'published_weekday', 'published_category', 'caps', 'link', 'duration_category']\n",
    "TARGET_COLUMN = \"view_count\"\n",
    "\n",
    "data['categoryId'] = data['categoryId'].replace(replacer)\n",
    "data['duration_category'] = data['duration_category'].fillna('Medium')\n",
    "data[categorical_columns[:-1]] = data[categorical_columns[:-1]].fillna('NaN')\n",
    "data['title'] = data['title'].fillna('NaN')\n",
    "data['description'] = data['description'].str.replace(r'http\\S+', '').fillna('NaN')\n",
    "data['tags'] = data['tags'].str.replace('|', ', ').replace('[None]', 'NaN')\n",
    "\n",
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "WR0TWtMpGBVI"
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "tok_mwe = nltk.tokenize.ToktokTokenizer()\n",
    "data[\"description\"] = data[\"description\"].apply(lambda x: ' '.join(tokenizer.tokenize(x.lower())))\n",
    "data[\"title\"] = data[\"title\"].apply(lambda x: ' '.join(tokenizer.tokenize(str(x).lower())))\n",
    "data[\"tags\"] = data[\"tags\"].apply(lambda x: ' '.join(''.join(str(x).lower().split(' ')).split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "8cZ4y9w4GBVJ"
   },
   "outputs": [],
   "source": [
    "token_counts = Counter()\n",
    "\n",
    "for row in data.iterrows():\n",
    "    token_counts.update([*row[1]['description'].split(), *row[1]['title'].split(), *row[1]['tags'].split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uxBSW7IVGBVK",
    "outputId": "3c519aaf-2515-4379-e38c-874eae1a1485"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens : 715794\n",
      "(':', 458538)\n",
      "(',', 405677)\n",
      "('-', 329752)\n",
      "('.', 326291)\n",
      "('в', 210637)\n",
      "...\n",
      "('другаяжизньаннысмотретьонлайн', 1)\n",
      "('другаяжизньанны2019', 1)\n",
      "('другаяжизньаннысериал', 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Total unique tokens :\", len(token_counts))\n",
    "print('\\n'.join(map(str, token_counts.most_common(n=5))))\n",
    "print('...')\n",
    "print('\\n'.join(map(str, token_counts.most_common()[-3:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "gkhkMd3BGBVL",
    "outputId": "06b64829-6bc6-4af8-b68f-53262f55017f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(list(token_counts.values()), range=[0, 10**4], bins=50, log=True)\n",
    "plt.xlabel(\"Word counts\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bD3w_9heGBVM",
    "outputId": "786cd5cf-36fd-4e3d-fbd2-5b26e94082d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 128605\n"
     ]
    }
   ],
   "source": [
    "min_count = 5\n",
    "tokens = sorted(t for t, c in token_counts.items() if c >= min_count)\n",
    "UNK, PAD = \"UNK\", \"PAD\"\n",
    "tokens = [UNK, PAD] + tokens\n",
    "print(\"Vocabulary size:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "FUfUfkX4GBVM"
   },
   "outputs": [],
   "source": [
    "token_to_id = dict(zip(tokens, [i for i in range(len(tokens))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "QzkbCU2wGBVM"
   },
   "outputs": [],
   "source": [
    "UNK_IX, PAD_IX = map(token_to_id.get, [UNK, PAD])\n",
    "\n",
    "def as_matrix(sequences, max_len=None):\n",
    "    \"\"\"\n",
    "    Convert a list of tokens into a matrix with padding\n",
    "    \"\"\"\n",
    "    if isinstance(sequences[0], str):\n",
    "        sequences = list(map(str.split, sequences))\n",
    "        \n",
    "    max_len = min(max(map(len, sequences)), max_len or float('inf'))\n",
    "    \n",
    "    matrix = np.full((len(sequences), max_len), np.int32(PAD_IX))\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = [token_to_id.get(word, UNK_IX) for word in seq[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "viJH-KElGBVO",
    "outputId": "f8f3e5a0-5b17-45ac-f682-324c51e724a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictVectorizer(dtype=<class 'numpy.float32'>, sparse=False)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_vectorizer = DictVectorizer(dtype=np.float32, sparse=False)\n",
    "categorical_vectorizer.fit(data[categorical_columns].apply(dict, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WghRdDEaGBVO",
    "outputId": "597c1f62-214b-495d-ba84-677a747cf5fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size =  58127\n",
      "Validation size =  14532\n"
     ]
    }
   ],
   "source": [
    "data_train, data_val = train_test_split(data, test_size=0.2, random_state=42)\n",
    "data_train.index = range(len(data_train))\n",
    "data_val.index = range(len(data_val))\n",
    "\n",
    "print(\"Train size = \", len(data_train))\n",
    "print(\"Validation size = \", len(data_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "O6d_uIilGBVO"
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "6nsDvs_8GBVP"
   },
   "outputs": [],
   "source": [
    "def to_tensors(batch, device):\n",
    "    batch_tensors = dict()\n",
    "    for key, arr in batch.items():\n",
    "        if key in [\"description\", \"title\", 'tags']:\n",
    "            batch_tensors[key] = torch.tensor(arr, dtype=torch.int64, device=device)\n",
    "        else:\n",
    "            batch_tensors[key] = torch.tensor(arr, device=device)\n",
    "    return batch_tensors\n",
    "\n",
    "\n",
    "def make_batch(data, max_len=None, word_dropout=0, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Creates a keras-friendly dict from the batch data.\n",
    "    :param word_dropout: replaces token index with UNK_IX with this probability\n",
    "    :returns: a dict with {'title' : int64[batch, title_max_len]\n",
    "    \"\"\"\n",
    "    batch = {}\n",
    "    batch[\"title\"] = as_matrix(data[\"title\"].values, max_len)\n",
    "    batch[\"description\"] = as_matrix(data[\"description\"].values, max_len)\n",
    "    batch[\"tags\"] = as_matrix(data[\"tags\"].values, max_len)\n",
    "    batch['Categorical'] = categorical_vectorizer.transform(data[categorical_columns].apply(dict, axis=1))\n",
    "    \n",
    "    if word_dropout != 0:\n",
    "        batch[\"description\"] = apply_word_dropout(batch[\"description\"], 1. - word_dropout)\n",
    "    \n",
    "    if TARGET_COLUMN in data.columns:\n",
    "        batch[TARGET_COLUMN] = data[TARGET_COLUMN].values\n",
    "    \n",
    "    return to_tensors(batch, device)\n",
    "\n",
    "\n",
    "def apply_word_dropout(matrix, keep_prop, replace_with=UNK_IX, pad_ix=PAD_IX,):\n",
    "    dropout_mask = np.random.choice(2, np.shape(matrix), p=[keep_prop, 1 - keep_prop])\n",
    "    dropout_mask &= matrix != pad_ix\n",
    "    return np.choose(dropout_mask, [matrix, np.full_like(matrix, replace_with)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "xjKd9TQIGBVP"
   },
   "outputs": [],
   "source": [
    "class ViewsPredictor(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), n_cat_features=len(categorical_vectorizer.vocabulary_), hid_size=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=n_tokens,\n",
    "                                      embedding_dim=hid_size,\n",
    "                                      padding_idx=0,\n",
    "                                      max_norm=5.0).to(DEVICE)\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels=1,\n",
    "                                              out_channels=100,\n",
    "                                              kernel_size=(fs, hid_size)) for fs in [1, 2, 3, 4]]).to(DEVICE)\n",
    "\n",
    "        self.lstm = nn.LSTM(64, hidden_size=hid_size, batch_first=True, bidirectional=True).to(DEVICE)\n",
    "        self.fc = nn.Linear(hid_size * 2, 100).to(DEVICE)\n",
    "\n",
    "        self.hidden = nn.Linear(200, 100).to(DEVICE)\n",
    "                \n",
    "        self.max_pool = nn.AdaptiveMaxPool1d(2).to(DEVICE)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(2).to(DEVICE)\n",
    "        \n",
    "        self.categories1 = nn.Linear(n_cat_features, 300).to(DEVICE)\n",
    "        self.norm_cat = nn.BatchNorm1d(300, momentum=0.1).to(DEVICE)\n",
    "        self.categories2 = nn.Linear(300, 100).to(DEVICE)\n",
    "        \n",
    "        self.relu = nn.ReLU().to(DEVICE)\n",
    "        self.dropout = nn.Dropout(0.5).to(DEVICE)\n",
    "        \n",
    "        self.fc1 = nn.Linear(100 * 49, hid_size, bias=True).to(DEVICE)\n",
    "        self.fc2 = nn.Linear(hid_size, 1, bias=True).to(DEVICE)\n",
    "        \n",
    "        \n",
    "    def forward(self, batch):\n",
    "        # emb\n",
    "        x_embed_title = self.embedding(batch['title']).unsqueeze(1)\n",
    "        x_embed_description = self.embedding(batch['description']).unsqueeze(1)\n",
    "        x_embed_tags = self.embedding(batch['tags']).unsqueeze(1)\n",
    "            \n",
    "        # # conv\n",
    "        convolution_title = [conv(x_embed_title) for conv in self.convs]\n",
    "        convolution_description = [conv(x_embed_description) for conv in self.convs]\n",
    "        convolution_tags = [conv(x_embed_tags) for conv in self.convs]\n",
    "       \n",
    "        # # pooling\n",
    "        max_title = torch.cat([self.max_pool(conv_title.squeeze().unsqueeze(2 if batch['title'].shape[0] != 1 else 0))\n",
    "                     if len(conv_title.squeeze().shape) < 3 else self.max_pool(conv_title.squeeze())\n",
    "                     for conv_title in convolution_title], dim=2)\n",
    "        \n",
    "        avg_title = torch.cat([self.avg_pool(conv_title.squeeze().unsqueeze(2 if batch['title'].shape[0] != 1 else 0))\n",
    "                     if len(conv_title.squeeze().shape) < 3 else self.avg_pool(conv_title.squeeze())\n",
    "                     for conv_title in convolution_title], dim=2)\n",
    "        \n",
    "        title = torch.cat([max_title, avg_title], dim=-1)\n",
    "\n",
    "        # h0 = torch.randn(2, batch['title'].shape[0], 64, device=DEVICE)\n",
    "        # c0 = torch.randn(2, batch['title'].shape[0], 64, device=DEVICE)\n",
    "        # recurrent_title, (_,_) = self.lstm(x_embed_title.squeeze(1), (h0, c0))\n",
    "        # title = self.max_pool(self.fc(recurrent_title).permute(0, 2, 1)) \n",
    "\n",
    "        # title = torch.cat([title1, title2], dim=2)\n",
    "        \n",
    "        max_description = torch.cat([self.max_pool(conv_description.squeeze().unsqueeze(2 if batch['description'].shape[0] != 1 else 0)) \n",
    "                     if len(conv_description.squeeze().shape) < 3 else self.max_pool(conv_description.squeeze())\n",
    "                     for conv_description in convolution_description], dim=2)\n",
    "        \n",
    "        avg_description = torch.cat([self.avg_pool(conv_description.squeeze().unsqueeze(2 if batch['description'].shape[0] != 1 else 0)) \n",
    "                     if len(conv_description.squeeze().shape) < 3 else self.avg_pool(conv_description.squeeze())\n",
    "                     for conv_description in convolution_description], dim=2)\n",
    "        \n",
    "        description = torch.cat([max_description, avg_description], dim=-1) \n",
    "\n",
    "        # h0 = torch.randn(2, batch['description'].shape[0], 64, device=DEVICE)\n",
    "        # c0 = torch.randn(2, batch['description'].shape[0], 64, device=DEVICE)\n",
    "        # recurrent_description, (_,_) = self.lstm(x_embed_description.squeeze(1), (h0, c0))\n",
    "        # description = self.max_pool(self.fc(recurrent_description).permute(0, 2, 1)) \n",
    "\n",
    "        # description = torch.cat([description1, description2], dim=2)\n",
    "        \n",
    "        max_tags = torch.cat([self.max_pool(conv_tags.squeeze().unsqueeze(2 if batch['tags'].shape[0] != 1 else 0)) \n",
    "                     if len(conv_tags.squeeze().shape) < 3 else self.max_pool(conv_tags.squeeze())\n",
    "                     for conv_tags in convolution_tags], dim=2)\n",
    "        \n",
    "        avg_tags = torch.cat([self.avg_pool(conv_tags.squeeze().unsqueeze(2 if batch['tags'].shape[0] != 1 else 0)) \n",
    "                     if len(conv_tags.squeeze().shape) < 3 else self.avg_pool(conv_tags.squeeze())\n",
    "                     for conv_tags in convolution_tags], dim=2)\n",
    "        \n",
    "        tags = torch.cat([max_tags, avg_tags], dim=-1)  \n",
    "\n",
    "        # h0 = torch.randn(2, batch['tags'].shape[0], 64, device=DEVICE)\n",
    "        # c0 = torch.randn(2, batch['tags'].shape[0], 64, device=DEVICE)\n",
    "        # recurrent_tags, (_,_) = self.lstm(x_embed_tags.squeeze(1), (h0, c0))\n",
    "        # tags = self.max_pool(self.fc(recurrent_tags).permute(0, 2, 1)) \n",
    "\n",
    "        # tags = torch.cat([tags1, tags2], dim=2)\n",
    "        \n",
    "        # categories\n",
    "        categories = self.categories2(self.norm_cat(self.categories1(batch['Categorical']))).unsqueeze(1).permute(0, 2, 1)\n",
    "        \n",
    "        #concat\n",
    "        cat = torch.cat((title, description, tags, categories), dim=2)\n",
    "        \n",
    "        out = cat.view(cat.shape[0], -1)        \n",
    "        out = self.fc1(self.relu(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "c6IPbxJWGBVQ"
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(data, batch_size=256, shuffle=True, cycle=False, device=DEVICE, **kwargs):\n",
    "    \"\"\" iterates minibatches of data in random order \"\"\"\n",
    "    while True:\n",
    "        indices = np.arange(len(data))\n",
    "        if shuffle:\n",
    "            indices = np.random.permutation(indices)\n",
    "\n",
    "        for start in range(0, len(indices), batch_size):\n",
    "            batch = make_batch(data.iloc[indices[start : start + batch_size]], **kwargs)\n",
    "            yield batch\n",
    "        \n",
    "        if not cycle: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "HUIp4-AbGBVQ"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "rCDqtdd7GBVR"
   },
   "outputs": [],
   "source": [
    "def print_metrics(model, data, batch_size=BATCH_SIZE, name=\"\", **kw):\n",
    "    valid_losses = []\n",
    "    squared_error = abs_error = num_samples = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterate_minibatches(data, batch_size=batch_size, shuffle=False, **kw):\n",
    "            batch_pred = model(batch)\n",
    "            squared_error += torch.sum(torch.square(batch_pred - batch[TARGET_COLUMN]))\n",
    "            abs_error += torch.sum(torch.abs(batch_pred - batch[TARGET_COLUMN]))\n",
    "            num_samples += len(batch_pred)\n",
    "            loss = criterion(batch_pred, batch[TARGET_COLUMN])\n",
    "            valid_losses.append(loss.item())\n",
    "    valid_loss = np.average(valid_losses)\n",
    "    mse = squared_error.detach().cpu().numpy() / num_samples\n",
    "    mae = abs_error.detach().cpu().numpy() / num_samples\n",
    "    print(\"%s results:\" % (name or \"\"))\n",
    "    print(\"Mean square error: %.5f\" % mse)\n",
    "    print(\"Mean absolute error: %.5f\" % mae)\n",
    "    return mse, mae, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "txnDq9tAYlrM",
    "outputId": "2ceda9f6-e46c-4b90-9b7c-014661000d63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1817it [01:41, 17.86it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " results:\n",
      "Mean square error: 0.74541\n",
      "Mean absolute error: 0.64743\n",
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1817it [01:40, 18.01it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " results:\n",
      "Mean square error: 0.72570\n",
      "Mean absolute error: 0.63544\n",
      "epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1817it [01:41, 17.84it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " results:\n",
      "Mean square error: 0.70863\n",
      "Mean absolute error: 0.62861\n",
      "epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1817it [01:41, 17.97it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " results:\n",
      "Mean square error: 0.72329\n",
      "Mean absolute error: 0.61618\n",
      "epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1817it [01:41, 17.97it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " results:\n",
      "Mean square error: 0.68397\n",
      "Mean absolute error: 0.61300\n",
      "epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1817it [01:41, 17.98it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " results:\n",
      "Mean square error: 0.70713\n",
      "Mean absolute error: 0.60501\n",
      "epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1817it [01:41, 17.94it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " results:\n",
      "Mean square error: 0.68395\n",
      "Mean absolute error: 0.59850\n",
      "epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1817it [01:40, 17.99it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " results:\n",
      "Mean square error: 0.67603\n",
      "Mean absolute error: 0.59653\n",
      "epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1817it [01:40, 18.02it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " results:\n",
      "Mean square error: 0.73333\n",
      "Mean absolute error: 0.60488\n",
      "epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1817it [01:41, 17.97it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " results:\n",
      "Mean square error: 0.64826\n",
      "Mean absolute error: 0.59031\n",
      "epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1817it [01:41, 17.97it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " results:\n",
      "Mean square error: 0.66120\n",
      "Mean absolute error: 0.58500\n",
      "epoch: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1817it [01:40, 17.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " results:\n",
      "Mean square error: 0.68084\n",
      "Mean absolute error: 0.58701\n",
      "epoch: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1817it [01:40, 18.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " results:\n",
      "Mean square error: 0.65028\n",
      "Mean absolute error: 0.60857\n",
      "epoch: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1817it [01:41, 17.98it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " results:\n",
      "Mean square error: 0.63949\n",
      "Mean absolute error: 0.59801\n",
      "epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 498/1816 [00:27<01:13, 17.84it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-b586ed497154>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTARGET_COLUMN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ViewsPredictor().to(DEVICE)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    model.train()\n",
    "    for i, batch in tqdm(enumerate(\n",
    "            iterate_minibatches(data_train, batch_size=BATCH_SIZE, device=DEVICE)),\n",
    "            total=len(data_train) // BATCH_SIZE\n",
    "        ):\n",
    "        pred = model(batch)\n",
    "        loss = criterion(pred, batch[TARGET_COLUMN].float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    mse, mae, valid_loss = print_metrics(model, data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "6PxQgmTe9Ljd"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Data3_ru.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "jo3uesP79ZEP"
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in iterate_minibatches(data_val, batch_size=BATCH_SIZE, shuffle=False):\n",
    "        preds.extend(model(batch).detach().cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "N037nlQ_-onB"
   },
   "outputs": [],
   "source": [
    "from math import ceil, floor\n",
    "\n",
    "preds_round = []\n",
    "preds_ceil = []\n",
    "preds_floor = []\n",
    "for val in preds:\n",
    "    preds_round.append(round(val))\n",
    "    preds_ceil.append(ceil(val))\n",
    "    preds_floor.append(floor(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o-zqIF1Q_o7J",
    "outputId": "7acc4351-8a66-431e-98ea-ee6019cc612a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float 0.8020204124283444\n",
      "Round 0.8631999945918464\n",
      "Ceil 0.9364988001874568\n",
      "Floor 1.0147209883918642\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "print('Float', mean_squared_error(preds, data_val['view_count'], squared=False))\n",
    "print('Round', mean_squared_error(preds_round, data_val['view_count'], squared=False))\n",
    "print('Ceil', mean_squared_error(preds_ceil, data_val['view_count'], squared=False))\n",
    "print('Floor', mean_squared_error(preds_floor, data_val['view_count'], squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "on-jtOx1CTZ1"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "dayss = [f'Day{i}' for i in range(1, 11)]\n",
    "X = pd.DataFrame(columns=['video_id', 'pred', *dayss, 'view_count'])\n",
    "X['pred'] = preds\n",
    "X['video_id'] = data_val['video_id']\n",
    "for i in range(1, 11):\n",
    "    X[f'Day{i}'] = data_val[f'Day{i}'].fillna(data_val[f'Day{i}'].mean())\n",
    "X['view_count'] = data_val['view_count']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X[['pred', *dayss]].fillna(0))\n",
    "\n",
    "y = X['view_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "R_H1ZHL1xD2r"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, HuberRegressor, TheilSenRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "\n",
    "def get_models(X_valid, y_valid):\n",
    "    models = []\n",
    "    max_depth = [3, 5, 7]\n",
    "    max_features = [int(np.sqrt(X_train.shape[1])), X_train.shape[1] // 2, X_train.shape[1] - 1]\n",
    "    alpha = np.logspace(-3, 0, 8, dtype='float')\n",
    "    n_estimators = [25, 50, 100]\n",
    "\n",
    "    print('Searching DecisionTreeRegressor...')\n",
    "    searcher = GridSearchCV(DecisionTreeRegressor(random_state=21), \n",
    "                            [{\"max_depth\": max_depth, \"max_features\": max_features}], \n",
    "                            scoring='neg_mean_squared_error', cv=4)\n",
    "    searcher.fit(X_valid, y_valid)\n",
    "    models.append(DecisionTreeRegressor(random_state=21, max_depth=searcher.best_params_[\"max_depth\"], \n",
    "                                        max_features=searcher.best_params_[\"max_features\"]))\n",
    "    print('Finished!\\n')\n",
    "\n",
    "    print('Searching GradientBoostingRegressor...')\n",
    "    searcher = GridSearchCV(GradientBoostingRegressor(max_features='sqrt', loss='huber', random_state=21), \n",
    "                            [{\"n_estimators\": n_estimators, \"max_depth\": max_depth, \n",
    "                              \"alpha\": np.logspace(-4, -0.01, 8, dtype='float')}], \n",
    "                            scoring='neg_mean_squared_error', cv=4)\n",
    "    searcher.fit(X_valid, y_valid)\n",
    "    models.append(GradientBoostingRegressor(warm_start=False, \n",
    "                                      alpha=searcher.best_params_[\"alpha\"], \n",
    "                                      max_depth=searcher.best_params_[\"max_depth\"], \n",
    "                                      n_estimators=searcher.best_params_[\"n_estimators\"], \n",
    "                                      max_features='sqrt', random_state=21))\n",
    "    print('Finished!\\n')\n",
    "\n",
    "    print('Searching BaggingRegressor...')\n",
    "    searcher = GridSearchCV(BaggingRegressor(random_state=21), \n",
    "                            [{\"n_estimators\": n_estimators, \"max_features\": max_features}], \n",
    "                            scoring='neg_mean_squared_error', cv=4)\n",
    "    searcher.fit(X_valid, y_valid)\n",
    "    models.append(BaggingRegressor(random_state=21, n_estimators=searcher.best_params_[\"n_estimators\"], \n",
    "                                   max_features=searcher.best_params_[\"max_features\"]))\n",
    "    print('Finished!\\n')\n",
    "\n",
    "    print('Searching CatBoostRegressor...')\n",
    "    models.append(CatBoostRegressor(loss_function='RMSE', random_seed=21, logging_level='Silent'))\n",
    "    print('Finished!\\n')\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "pKk8CLgTyk43"
   },
   "outputs": [],
   "source": [
    "def testing(model, X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame, y_test: pd.Series):\n",
    "\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    fin = time.time() - start\n",
    "\n",
    "    test = y_test.copy()\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    MSE = mean_squared_error(y_pred, test)\n",
    "    R2 = r2_score(test, y_pred)\n",
    "\n",
    "    return MSE, R2, fin, y_pred, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXmh8xoyEn5N"
   },
   "source": [
    "## Russian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Clkc2FskFA9p"
   },
   "source": [
    "### Prediction, having 1 day in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m2mttrulEOq8",
    "outputId": "24e07a85-72f0-421c-bcaa-176df01b2f17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching DecisionTreeRegressor...\n",
      "Finished!\n",
      "\n",
      "Searching GradientBoostingRegressor...\n",
      "Finished!\n",
      "\n",
      "Searching BaggingRegressor...\n",
      "Finished!\n",
      "\n",
      "Searching CatBoostRegressor...\n",
      "Finished!\n",
      "\n",
      "DecisionTree \n",
      "\tRMSE 0.7428692847841711 \n",
      "\tR2 0.2583941877697875 \n",
      "\tTime 0.005785465240478516\n",
      "Float 0.7428692847841711\n",
      "Round 0.8175914760043749\n",
      "Ceil 0.9288625176302514\n",
      "Floor 0.9086397549298187\n",
      "GradientBoosting \n",
      "\tRMSE 0.7212999487705276 \n",
      "\tR2 0.30083428533626666 \n",
      "\tTime 0.15889930725097656\n",
      "Float 0.7212999487705276\n",
      "Round 0.7992048071048009\n",
      "Ceil 0.8964406480061816\n",
      "Floor 0.9209761572924288\n",
      "Bagging \n",
      "\tRMSE 0.3228328729622158 \n",
      "\tR2 0.8599433214643017 \n",
      "\tTime 0.9894311428070068\n",
      "Float 0.3228328729622158\n",
      "Round 0.32085078203505685\n",
      "Ceil 0.7626355564865936\n",
      "Floor 0.6671481883377277\n",
      "CatBoost \n",
      "\tRMSE 0.6541151849196115 \n",
      "\tR2 0.4250146452720853 \n",
      "\tTime 1.5200932025909424\n",
      "Float 0.6541151849196115\n",
      "Round 0.7313136866147883\n",
      "Ceil 0.8567987197952472\n",
      "Floor 0.8604852713931909\n"
     ]
    }
   ],
   "source": [
    "ans = pd.DataFrame(index=['DecisionTree', 'GradientBoosting', 'Bagging', 'CatBoost'], \n",
    "                   columns=['MSE', 'R2', 'Time'])\n",
    "\n",
    "data = dict(zip(['MSE', 'R2', 'Time', 'Pred', 'Act'] , [dict(), dict(), dict(), dict(), dict()]))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.5, random_state=6)\n",
    "X_test, X_valid, y_test, y_valid = train_test_split(X_scaled, y, train_size=0.5, random_state=6)\n",
    "\n",
    "models_set = get_models(X_valid, y_valid)\n",
    "\n",
    "for j, model in enumerate(models_set):\n",
    "    curr_MSE, curr_R2, curr_fin, curr_pred, curr_test = testing(model, X_train, y_train, X_test, y_test)\n",
    "    try:\n",
    "        data['MSE'][ans.index.to_list()[j]].append(curr_MSE)\n",
    "        data['R2'][ans.index.to_list()[j]].append(curr_R2)\n",
    "        data['Time'][ans.index.to_list()[j]].append(curr_fin)\n",
    "        data['Pred'][ans.index.to_list()[j]].extend(curr_pred)\n",
    "        data['Act'][ans.index.to_list()[j]].extend(curr_test)\n",
    "    except KeyError:\n",
    "        data['MSE'][ans.index.to_list()[j]] = [curr_MSE]\n",
    "        data['R2'][ans.index.to_list()[j]] = [curr_R2]\n",
    "        data['Time'][ans.index.to_list()[j]] = [curr_fin]\n",
    "        data['Pred'][ans.index.to_list()[j]] = curr_pred.tolist()\n",
    "        data['Act'][ans.index.to_list()[j]] = curr_test.tolist()\n",
    "\n",
    "for m in ans.index.to_list():\n",
    "    preds_round = []\n",
    "    preds_ceil = []\n",
    "    preds_floor = []\n",
    "    for val in data['Pred'][m]:\n",
    "        preds_round.append(round(val))\n",
    "        preds_ceil.append(ceil(val))\n",
    "        preds_floor.append(floor(val))\n",
    "\n",
    "    print(m, '\\n\\tRMSE', mean_squared_error(data['Act'][m], data['Pred'][m], squared=False), \n",
    "        '\\n\\tR2', r2_score(data['Act'][m], data['Pred'][m]),\n",
    "        '\\n\\tTime', np.sum(data['Time'][m])) \n",
    "    print('Float', mean_squared_error(data['Pred'][m], data['Act'][m], squared=False))\n",
    "    print('Round', mean_squared_error(preds_round, data['Act'][m], squared=False))\n",
    "    print('Ceil', mean_squared_error(preds_ceil, data['Act'][m], squared=False))\n",
    "    print('Floor', mean_squared_error(preds_floor, data['Act'][m], squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNe5se6vFEYm"
   },
   "source": [
    "### Prediction, having 10 days in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4PeUOvQ6EyGo",
    "outputId": "53e84817-6456-420a-967f-6f6a101ef28c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching DecisionTreeRegressor...\n",
      "Finished!\n",
      "\n",
      "Searching GradientBoostingRegressor...\n",
      "Finished!\n",
      "\n",
      "Searching BaggingRegressor...\n",
      "Finished!\n",
      "\n",
      "Searching CatBoostRegressor...\n",
      "Finished!\n",
      "\n",
      "DecisionTree \n",
      "\tRMSE 0.6582898653648368 \n",
      "\tR2 0.41765190549648534 \n",
      "\tTime 0.010411500930786133\n",
      "Float 0.6582898653648368\n",
      "Round 0.715138494514427\n",
      "Ceil 0.9511173836370815\n",
      "Floor 0.7633570661621435\n",
      "GradientBoosting \n",
      "\tRMSE 0.6214804895171349 \n",
      "\tR2 0.48095702310887045 \n",
      "\tTime 0.18789410591125488\n",
      "Float 0.6214804895171349\n",
      "Round 0.680930533470233\n",
      "Ceil 0.9223201037577794\n",
      "Floor 0.7596520854602608\n",
      "Bagging \n",
      "\tRMSE 0.26964223565004214 \n",
      "\tR2 0.9022933418689628 \n",
      "\tTime 1.6195454597473145\n",
      "Float 0.26964223565004214\n",
      "Round 0.25995193186696125\n",
      "Ceil 0.8133722741599594\n",
      "Floor 0.5815069122696145\n",
      "CatBoost \n",
      "\tRMSE 0.5685039807069739 \n",
      "\tR2 0.5656744875509758 \n",
      "\tTime 2.041318416595459\n",
      "Float 0.5685039807069739\n",
      "Round 0.631322956992159\n",
      "Ceil 0.9017217527932333\n",
      "Floor 0.7134043557770927\n"
     ]
    }
   ],
   "source": [
    "ans = pd.DataFrame(index=['DecisionTree', 'GradientBoosting', 'Bagging', 'CatBoost'], \n",
    "                   columns=['MSE', 'R2', 'Time'])\n",
    "\n",
    "data = dict(zip(['MSE', 'R2', 'Time', 'Pred', 'Act'] , [dict(), dict(), dict(), dict(), dict()]))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.5, random_state=6)\n",
    "X_test, X_valid, y_test, y_valid = train_test_split(X_scaled, y, train_size=0.5, random_state=6)\n",
    "\n",
    "models_set = get_models(X_valid, y_valid)\n",
    "\n",
    "for j, model in enumerate(models_set):\n",
    "    curr_MSE, curr_R2, curr_fin, curr_pred, curr_test = testing(model, X_train, y_train, X_test, y_test)\n",
    "    try:\n",
    "        data['MSE'][ans.index.to_list()[j]].append(curr_MSE)\n",
    "        data['R2'][ans.index.to_list()[j]].append(curr_R2)\n",
    "        data['Time'][ans.index.to_list()[j]].append(curr_fin)\n",
    "        data['Pred'][ans.index.to_list()[j]].extend(curr_pred)\n",
    "        data['Act'][ans.index.to_list()[j]].extend(curr_test)\n",
    "    except KeyError:\n",
    "        data['MSE'][ans.index.to_list()[j]] = [curr_MSE]\n",
    "        data['R2'][ans.index.to_list()[j]] = [curr_R2]\n",
    "        data['Time'][ans.index.to_list()[j]] = [curr_fin]\n",
    "        data['Pred'][ans.index.to_list()[j]] = curr_pred.tolist()\n",
    "        data['Act'][ans.index.to_list()[j]] = curr_test.tolist()\n",
    "\n",
    "for m in ans.index.to_list():\n",
    "    preds_round = []\n",
    "    preds_ceil = []\n",
    "    preds_floor = []\n",
    "    for val in data['Pred'][m]:\n",
    "        preds_round.append(round(val))\n",
    "        preds_ceil.append(ceil(val))\n",
    "        preds_floor.append(floor(val))\n",
    "\n",
    "        print(m, '\\n\\tRMSE', mean_squared_error(data['Act'][m], data['Pred'][m], squared=False), \n",
    "            '\\n\\tR2', r2_score(data['Act'][m], data['Pred'][m]),\n",
    "            '\\n\\tTime', np.sum(data['Time'][m])) \n",
    "        print('Float', mean_squared_error(data['Pred'][m], data['Act'][m], squared=False))\n",
    "        print('Round', mean_squared_error(preds_round, data['Act'][m], squared=False))\n",
    "        print('Ceil', mean_squared_error(preds_ceil, data['Act'][m], squared=False))\n",
    "        print('Floor', mean_squared_error(preds_floor, data['Act'][m], squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovUwg8chEkKx"
   },
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zahl5F2rEf7M"
   },
   "source": [
    "### Prediction, having 1 day in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qejbLu0Y4tdc",
    "outputId": "0fd76032-d719-4394-b2e6-9856a8b4f454"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching DecisionTreeRegressor...\n",
      "Finished!\n",
      "\n",
      "Searching GradientBoostingRegressor...\n",
      "Finished!\n",
      "\n",
      "Searching BaggingRegressor...\n",
      "Finished!\n",
      "\n",
      "Searching CatBoostRegressor...\n",
      "Finished!\n",
      "\n",
      "DecisionTree \n",
      "\tRMSE 1.6635274437245273 \n",
      "\tR2 0.30725158747343717 \n",
      "\tTime 0.005626201629638672\n",
      "Float 1.6635274437245273\n",
      "Round 1.7084998624017407\n",
      "Ceil 1.7254241716826848\n",
      "Floor 1.7621172715144224\n",
      "GradientBoosting \n",
      "\tRMSE 1.551324902468991 \n",
      "\tR2 0.39754983541857436 \n",
      "\tTime 0.15655255317687988\n",
      "Float 1.551324902468991\n",
      "Round 1.580255018483505\n",
      "Ceil 1.6522990663092454\n",
      "Floor 1.6550869032068396\n",
      "Bagging \n",
      "\tRMSE 0.7237596679832068 \n",
      "\tR2 0.8688693072651279 \n",
      "\tTime 0.27385568618774414\n",
      "Float 0.7237596679832068\n",
      "Round 0.7736221291165625\n",
      "Ceil 0.927446207759367\n",
      "Floor 0.9145527964785853\n",
      "CatBoost \n",
      "\tRMSE 1.4319882454734507 \n",
      "\tR2 0.4866725300908279 \n",
      "\tTime 1.5127933025360107\n",
      "Float 1.4319882454734507\n",
      "Round 1.4598089568693642\n",
      "Ceil 1.5414094202509026\n",
      "Floor 1.5444878960789545\n"
     ]
    }
   ],
   "source": [
    "ans = pd.DataFrame(index=['DecisionTree', 'GradientBoosting', 'Bagging', 'CatBoost'], \n",
    "                   columns=['MSE', 'R2', 'Time'])\n",
    "\n",
    "data = dict(zip(['MSE', 'R2', 'Time', 'Pred', 'Act'] , [dict(), dict(), dict(), dict(), dict()]))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.5, random_state=6)\n",
    "X_test, X_valid, y_test, y_valid = train_test_split(X_scaled, y, train_size=0.5, random_state=6)\n",
    "\n",
    "models_set = get_models(X_valid, y_valid)\n",
    "\n",
    "for j, model in enumerate(models_set):\n",
    "    curr_MSE, curr_R2, curr_fin, curr_pred, curr_test = testing(model, X_train, y_train, X_test, y_test)\n",
    "    try:\n",
    "        data['MSE'][ans.index.to_list()[j]].append(curr_MSE)\n",
    "        data['R2'][ans.index.to_list()[j]].append(curr_R2)\n",
    "        data['Time'][ans.index.to_list()[j]].append(curr_fin)\n",
    "        data['Pred'][ans.index.to_list()[j]].extend(curr_pred)\n",
    "        data['Act'][ans.index.to_list()[j]].extend(curr_test)\n",
    "    except KeyError:\n",
    "        data['MSE'][ans.index.to_list()[j]] = [curr_MSE]\n",
    "        data['R2'][ans.index.to_list()[j]] = [curr_R2]\n",
    "        data['Time'][ans.index.to_list()[j]] = [curr_fin]\n",
    "        data['Pred'][ans.index.to_list()[j]] = curr_pred.tolist()\n",
    "        data['Act'][ans.index.to_list()[j]] = curr_test.tolist()\n",
    "\n",
    "for m in ans.index.to_list():\n",
    "    preds_round = []\n",
    "    preds_ceil = []\n",
    "    preds_floor = []\n",
    "    for val in data['Pred'][m]:\n",
    "        preds_round.append(round(val))\n",
    "        preds_ceil.append(ceil(val))\n",
    "        preds_floor.append(floor(val))\n",
    "\n",
    "        print(m, '\\n\\tRMSE', mean_squared_error(data['Act'][m], data['Pred'][m], squared=False), \n",
    "            '\\n\\tR2', r2_score(data['Act'][m], data['Pred'][m]),\n",
    "            '\\n\\tTime', np.sum(data['Time'][m])) \n",
    "        print('Float', mean_squared_error(data['Pred'][m], data['Act'][m], squared=False))\n",
    "        print('Round', mean_squared_error(preds_round, data['Act'][m], squared=False))\n",
    "        print('Ceil', mean_squared_error(preds_ceil, data['Act'][m], squared=False))\n",
    "        print('Floor', mean_squared_error(preds_floor, data['Act'][m], squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "boN0hHIdEXII"
   },
   "source": [
    "### Prediction, having 10 days in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9Hz3gjZEHLp",
    "outputId": "ebe2432d-7904-44f3-8325-c090f6ea1da8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree \n",
      "\tRMSE 1.4723014809105333 \n",
      "\tR2 0.4573633821653903 \n",
      "\tTime 0.03257489204406738\n",
      "Float 1.4723014809105333\n",
      "Round 1.5028611996504935\n",
      "Ceil 1.5811830076964162\n",
      "Floor 1.5662246051670492\n",
      "GradientBoosting \n",
      "\tRMSE 1.0430141344224342 \n",
      "\tR2 0.7276696858987795 \n",
      "\tTime 2.283287525177002\n",
      "Float 1.0430141344224342\n",
      "Round 1.0783111607907478\n",
      "Ceil 1.1975223560494956\n",
      "Floor 1.1984552784666154\n",
      "Bagging \n",
      "\tRMSE 0.5383434823098683 \n",
      "\tR2 0.9274504842079773 \n",
      "\tTime 6.2538299560546875\n",
      "Float 0.5383434823098683\n",
      "Round 0.5777131163622546\n",
      "Ceil 0.8170952168883752\n",
      "Floor 0.8059051407186989\n",
      "CatBoost \n",
      "\tRMSE 1.0914759477975216 \n",
      "\tR2 0.7017750742949098 \n",
      "\tTime 3.745413303375244\n",
      "Float 1.0914759477975216\n",
      "Round 1.1247302616617127\n",
      "Ceil 1.2428617078835291\n",
      "Floor 1.2430864974870826\n"
     ]
    }
   ],
   "source": [
    "ans = pd.DataFrame(index=['DecisionTree', 'GradientBoosting', 'Bagging', 'CatBoost'], \n",
    "                   columns=['MSE', 'R2', 'Time'])\n",
    "\n",
    "data = dict(zip(['MSE', 'R2', 'Time', 'Pred', 'Act'] , [dict(), dict(), dict(), dict(), dict()]))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.5, random_state=6)\n",
    "X_test, X_valid, y_test, y_valid = train_test_split(X_scaled, y, train_size=0.5, random_state=6)\n",
    "\n",
    "models_set = get_models(X_valid, y_valid)\n",
    "\n",
    "for j, model in enumerate(models_set):\n",
    "    curr_MSE, curr_R2, curr_fin, curr_pred, curr_test = testing(model, X_train, y_train, X_test, y_test)\n",
    "    try:\n",
    "        data['MSE'][ans.index.to_list()[j]].append(curr_MSE)\n",
    "        data['R2'][ans.index.to_list()[j]].append(curr_R2)\n",
    "        data['Time'][ans.index.to_list()[j]].append(curr_fin)\n",
    "        data['Pred'][ans.index.to_list()[j]].extend(curr_pred)\n",
    "        data['Act'][ans.index.to_list()[j]].extend(curr_test)\n",
    "    except KeyError:\n",
    "        data['MSE'][ans.index.to_list()[j]] = [curr_MSE]\n",
    "        data['R2'][ans.index.to_list()[j]] = [curr_R2]\n",
    "        data['Time'][ans.index.to_list()[j]] = [curr_fin]\n",
    "        data['Pred'][ans.index.to_list()[j]] = curr_pred.tolist()\n",
    "        data['Act'][ans.index.to_list()[j]] = curr_test.tolist()\n",
    "\n",
    "for m in ans.index.to_list():\n",
    "    preds_round = []\n",
    "    preds_ceil = []\n",
    "    preds_floor = []\n",
    "    for val in data['Pred'][m]:\n",
    "        preds_round.append(round(val))\n",
    "        preds_ceil.append(ceil(val))\n",
    "        preds_floor.append(floor(val))\n",
    "\n",
    "        print(m, '\\n\\tRMSE', mean_squared_error(data['Act'][m], data['Pred'][m], squared=False), \n",
    "            '\\n\\tR2', r2_score(data['Act'][m], data['Pred'][m]),\n",
    "            '\\n\\tTime', np.sum(data['Time'][m])) \n",
    "        print('Float', mean_squared_error(data['Pred'][m], data['Act'][m], squared=False))\n",
    "        print('Round', mean_squared_error(preds_round, data['Act'][m], squared=False))\n",
    "        print('Ceil', mean_squared_error(preds_ceil, data['Act'][m], squared=False))\n",
    "        print('Floor', mean_squared_error(preds_floor, data['Act'][m], squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4ZeGliUGGJX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Experiments with static features (trends).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
